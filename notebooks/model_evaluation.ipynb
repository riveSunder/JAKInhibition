{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91461537",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "![eval summary](../assets/evaluation_summary.png)\n",
    "\n",
    "Frankly, this project is not actually ready to dust off the test set (saved to disk apart from training and validation used during development). I would normally undergo a few more rounds of development, making an effort to optimize over available hyperparameters, before letting myself or the models see the test set. This notebook does give an idea of how I would approach final evaluation, but like the rest of the project it should be though of as a prototype rather than a formal report. \n",
    "\n",
    "In any case I thought it was important to break out the performance of the whole system by measurement type and enzyme, for a finer level of granularity than the sparse loss used by the models during training. \n",
    "\n",
    "\n",
    "\n",
    "## xformer_x003 and ensemble_004_seed13\n",
    "\n",
    "\n",
    "|       |pKi:JAK1|pKi:JAK2|pKi:JAK3|pKi:TYK2|\n",
    "|-------|-------|-------|-------|-------|\n",
    "| test  |0.712+/-0.809|0.639+/-0.552|1.28+/-1.16|0.862+/-0.772|\n",
    "| train |0.618+/-0.591|.619+/-0.535|1.02+/-0.873|0.757+/-0.595|\n",
    "\n",
    "<div align=\"center\">\n",
    "    Evaluation, mean absolute error on train and test set\n",
    "    </div>\n",
    "\n",
    "\n",
    "Training a bootstrap ensemble of models to estimate kinase inhibition measurements gives some idea about the uncertainty of a given estimate in the variance of the population of estimates generated by the models. Here we can get an idea of how well the bootstrap ensemble provides uncertainty by counting the number of measurements that fall with 1, 2, or 3 standard deviations of the ensemble mean. \n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "evaluation on test set\n",
    "353.0 of 1047 within 1 boostrap ensemble standard deviations 33.715377268385865%\n",
    "606.0 of 1047 within 2 boostrap ensemble standard deviations 57.87965616045845%\n",
    "801.0 of 1047 within 3 boostrap ensemble standard deviations 76.50429799426934%\n",
    "\n",
    "evaluation on train set\n",
    "3384.0 of 9678 within 1 boostrap ensemble standard deviations 34.96590204587725%\n",
    "5932.0 of 9678 within 2 boostrap ensemble standard deviations 61.293655713990496%\n",
    "7703.0 of 9678 within 3 boostrap ensemble standard deviations 79.59289109320108%\n",
    "```\n",
    "\n",
    "## xformer_x_002_seed42 and ensemble_004_seed42\n",
    "\n",
    "It's evident from l see in the training \n",
    "\n",
    "\n",
    "|       |pKi:JAK1|pKi:JAK2|pKi:JAK3|pKi:TYK2|\n",
    "|-------|-------|-------|-------|-------|\n",
    "| test  |0.999+/-0.916|1.07+/-0.822|1.95+/-1.24|0.740+/-0.486|\n",
    "| train |0.969+/-0.812|1.06+/-0.795|1.72+/-1.13|0.897+/-0.611|\n",
    "\n",
    "<div align=\"center\">\n",
    "    Evaluation, mean absolute error on train and test set\n",
    "    </div>\n",
    "    \n",
    "\n",
    "```\n",
    "evaluation on test set\n",
    "199.0 of 1047 within 1 boostrap ensemble standard deviations 19.00668576886342%\n",
    "363.0 of 1047 within 2 boostrap ensemble standard deviations 34.67048710601719%\n",
    "495.0 of 1047 within 3 boostrap ensemble standard deviations 47.277936962750715%\n",
    "\n",
    "evaluation on train set\n",
    "1585.0 of 9678 within 1 boostrap ensemble standard deviations 16.377350692291795%\n",
    "3132.0 of 9678 within 2 boostrap ensemble standard deviations 32.36205827650341%\n",
    "4514.0 of 9678 within 3 boostrap ensemble standard deviations 46.641868154577395%\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import jak\n",
    "from jak.transformer import XFormer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "my_cmap = plt.get_cmap(\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../logs/ensemble_003.txt\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05555ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# might be useful to look at training curves for the transformers \n",
    "# the first run plotted here had too high a learning rate and a loss spiek as a result\n",
    "\n",
    "log_dir = \"../logs\"\n",
    "my_extension = \"txt\"\n",
    "\n",
    "log_listdir = os.listdir(log_dir)\n",
    "\n",
    "for filename in log_listdir:\n",
    "        \n",
    "    if \"xformer\" in filename and filename.endswith(my_extension):\n",
    "        \n",
    "        fig = plt.figure()\n",
    "\n",
    "        df = pd.read_csv(os.path.join(log_dir, filename))\n",
    "        print(df.columns)\n",
    "        \n",
    "        x = df[\"epoch\"].to_numpy()\n",
    "        train_loss = df[\" train_loss\"].to_numpy()\n",
    "        #train_std_dev = df[\" train_std_dev\"].to_numpy()\n",
    "        val_loss = df[\" val_loss\"].to_numpy()\n",
    "        #val_std_dev = df[\" val_std_dev\"].to_numpy()\n",
    "        \n",
    "        plt.plot(x, train_loss, label=\"training loss\", lw=3, color=my_cmap(200), alpha=0.5)\n",
    "        plt.plot(x, val_loss, label=\"val. loss\", lw=3, color=my_cmap(10), alpha=0.5)\n",
    "        plt.title(f\"Transformer training run {filename}\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac42282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# might be useful to look at training curves for the ensemble models as well\n",
    "\n",
    "log_dir = \"../logs\"\n",
    "my_extension = \"txt\"\n",
    "\n",
    "log_listdir = os.listdir(log_dir)\n",
    "\n",
    "for filename in log_listdir:\n",
    "        \n",
    "    if \"ensemble\" in filename and filename.endswith(my_extension):\n",
    "        \n",
    "        fig = plt.figure()\n",
    "\n",
    "        df = pd.read_csv(os.path.join(log_dir, filename))\n",
    "        \n",
    "        x = df[\"epoch\"].to_numpy()\n",
    "        train_loss = df[\" train_loss\"].to_numpy()\n",
    "        train_std_dev = df[\" train_std_dev\"].to_numpy()\n",
    "        val_loss = df[\" val_loss\"].to_numpy()\n",
    "        val_std_dev = df[\" val_std_dev\"].to_numpy()\n",
    "        \n",
    "        plt.plot(x, train_loss, label=\"training loss\", color=my_cmap(256), alpha=0.5)\n",
    "        plt.plot(x, val_loss, label=\"val. loss\", color=my_cmap(10), alpha=0.5)\n",
    "        plt.legend()\n",
    "        plt.fill_between(x, val_loss - val_std_dev, val_loss + val_std_dev, color=my_cmap(10), alpha=0.25)\n",
    "        \n",
    "        plt.fill_between(x, train_loss - train_std_dev, train_loss + train_std_dev, color=my_cmap(256), alpha=0.25)\n",
    "        plt.title(f\"MLP ensemble {filename}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a105abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up feature extractor model (pre-trained autoregressive transformer)\n",
    "\n",
    "seq_length = 100\n",
    "token_dim = 33\n",
    "encoder_size = 1\n",
    "decoder_size = 1\n",
    "smiles_vocab = \"#()+-1234567=BCFHINOPS[]cilnors\"\n",
    "parameters_fp = \"../parameters/xformer_x003_seed42/tag_xformer_x003_seed42_epoch299.pt\"\n",
    "\n",
    "\n",
    "\n",
    "xformer = XFormer(vocab = smiles_vocab, \\\n",
    "                  token_dim=token_dim, \\\n",
    "                  seq_length=seq_length, \\\n",
    "                  lr=1e-3,\\\n",
    "                  device=\"cpu\",\\\n",
    "                  tag=\"inference\"\n",
    "                 )\n",
    "\n",
    "model_state_dict = torch.load(parameters_fp, map_location=xformer.my_device)\n",
    "xformer.load_state_dict(model_state_dict)\n",
    "\n",
    "# set up regression model ensemble (multiple MLPs)\n",
    "\n",
    "kwarg_filepath = \"../parameters/ensemble_004_seed13/exp_tag_ensemble_004_seed13.json\"\n",
    "parameters_filepath = \"../parameters/ensemble_004_seed13/tag_ensemble_004_seed13_epoch4999.pt\"\n",
    "\n",
    "with open(kwarg_filepath, \"r\") as f:\n",
    "    kwargs = json.load(f)\n",
    "    \n",
    "in_dim = 3300\n",
    "out_dim = 8\n",
    "    \n",
    "ensemble = jak.mlp.MLPCohort(in_dim, out_dim,\\\n",
    "                             cohort_size=kwargs[\"cohort_size\"], \\\n",
    "                             depth=kwargs[\"depth\"], \\\n",
    "                             lr=kwargs[\"lr\"])\n",
    "\n",
    "ensemble.load_state_dict(torch.load(parameters_filepath))\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb89a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on the test set\n",
    "test_df = pd.read_csv(\"../data/test_JAK.csv\")\n",
    "train_df = pd.read_csv(\"../data/train_JAK.csv\")\n",
    "unique_smiles = test_df[\"SMILES\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1616d928",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses = {}\n",
    "losses[\"JAK1_pKi\"] = []\n",
    "losses[\"JAK2_pKi\"] = []\n",
    "losses[\"JAK3_pKi\"] = []\n",
    "losses[\"TYK2_pKi\"] = []\n",
    "\n",
    "losses[\"JAK1_pIC50\"] = []\n",
    "losses[\"JAK2_pIC50\"] = []\n",
    "losses[\"JAK3_pIC50\"] = []\n",
    "losses[\"TYK2_pIC50\"] = []\n",
    "\n",
    "std_dev_diff = {}\n",
    "std_dev_diff[\"JAK1_pKi\"] = []\n",
    "std_dev_diff[\"JAK2_pKi\"] = []\n",
    "std_dev_diff[\"JAK3_pKi\"] = []\n",
    "std_dev_diff[\"TYK2_pKi\"] = []\n",
    "\n",
    "std_dev_diff[\"JAK1_pIC50\"] = []\n",
    "std_dev_diff[\"JAK2_pIC50\"] = []\n",
    "std_dev_diff[\"JAK3_pIC50\"] = []\n",
    "std_dev_diff[\"TYK2_pIC50\"] = []\n",
    "\n",
    "\n",
    "loss_summary = \"\"\n",
    "for label, df in zip([\"test\", \"train\"], [test_df, train_df]):\n",
    "    within_1sd = 0\n",
    "    within_2sd = 0\n",
    "    within_3sd = 0\n",
    "\n",
    "    total_count = 0\n",
    "    unique_smiles = df[\"SMILES\"].unique()\n",
    "    with torch.no_grad():\n",
    "        for smile in unique_smiles:\n",
    "            s_df = df.loc[df[\"SMILES\"] == smile]\n",
    "            xformer.eval()\n",
    "            encoded = xformer.encode(smile).reshape(1, -1)\n",
    "\n",
    "            ensemble.eval()\n",
    "            mean_estimates, std_estimates = ensemble.forward(encoded)\n",
    "\n",
    "            for ii, measurement_type in enumerate([\"pIC50\", \"pKi\"]):\n",
    "                m_df = s_df.loc[s_df[\"measurement_type\"] == measurement_type]\n",
    "\n",
    "                if len(m_df):\n",
    "                    for jj, enzyme in enumerate([\"JAK1\", \"JAK2\", \"JAK3\", \"TYK2\"]):\n",
    "\n",
    "                        e_df = m_df.loc[m_df[\"Kinase_name\"] == enzyme]\n",
    "                        if len(e_df):\n",
    "                            idx = ii * 4 + jj\n",
    "                            loss = np.abs(\\\n",
    "                                    e_df[\"measurement_value\"].to_numpy() - mean_estimates[0,idx].cpu().numpy()).item()\n",
    "                            losses[f\"{enzyme}_{measurement_type}\"].append(loss)\n",
    "                            std_devs = loss / std_estimates[0,idx].numpy()\n",
    "                            std_dev_diff[f\"{enzyme}_{measurement_type}\"].append(std_devs)\n",
    "\n",
    "                            within_1sd += 1.0 *(std_devs <= 1.0)\n",
    "                            within_2sd += 1.0 *(std_devs <= 2.0)\n",
    "                            within_3sd += 1.0 *(std_devs <= 3.0)\n",
    "                            total_count += 1\n",
    "    \n",
    "    print(f\"evaluation on {label} set\")\n",
    "    print(f\"{within_1sd} of {total_count} within 1 boostrap ensemble standard deviations {100.0 * within_1sd / total_count}%\")\n",
    "    print(f\"{within_2sd} of {total_count} within 2 boostrap ensemble standard deviations {100.0 * within_2sd / total_count}%\")\n",
    "    print(f\"{within_3sd} of {total_count} within 3 boostrap ensemble standard deviations {100.0 * within_3sd / total_count}%\")                        \n",
    "    \n",
    "    fig, ax = plt.subplots(2,2, figsize=(12, 12))\n",
    "\n",
    "    all_losses = []\n",
    "    x_ticklabels = []\n",
    "\n",
    "    for ii, measurement_type in enumerate([\"pIC50\", \"pKi\"]):\n",
    "        for jj, enzyme in enumerate([\"JAK1\", \"JAK2\", \"JAK3\", \"TYK2\"]):\n",
    "            idx = 1.0* ii*4 + jj\n",
    "\n",
    "            all_losses.append(losses[f\"{enzyme}_{measurement_type}\"])\n",
    "            x_ticklabels.append(f\"{measurement_type}:{enzyme}\")\n",
    "            mean_loss = np.mean(losses[f\"{enzyme}_{measurement_type}\"])\n",
    "            \n",
    "            std_dev_of_loss = np.std(losses[f\"{enzyme}_{measurement_type}\"])\n",
    "            loss_summary += f\"\\n{label} set MAE {measurement_type}:{enzyme}: {mean_loss}$\\pm${std_dev_of_loss} s.d.\"\n",
    "    \n",
    "    ax[0,0].boxplot(all_losses[:4])\n",
    "    ax[0,0].set_xticklabels(x_ticklabels[:4])\n",
    "    ax[0,0].set_title(f\"{label} set losses, pCI50\")\n",
    "    \n",
    "    ax[0,1].boxplot(all_losses[4:])\n",
    "    ax[0,1].set_xticklabels(x_ticklabels[4:])\n",
    "    ax[0,1].set_title(f\"{label} set losses, pKi\")\n",
    "\n",
    "    all_std_devs = []\n",
    "    \n",
    "    for ii, measurement_type in enumerate([\"pIC50\", \"pKi\"]):\n",
    "        for jj, enzyme in enumerate([\"JAK1\", \"JAK2\", \"JAK3\", \"TYK2\"]):\n",
    "            idx = 1.0* ii*4 + jj\n",
    "\n",
    "            all_std_devs.append(std_dev_diff[f\"{enzyme}_{measurement_type}\"])\n",
    "            mean_sigma = np.mean(losses[f\"{enzyme}_{measurement_type}\"])\n",
    "            \n",
    "            std_dev_of_sigma = np.std(losses[f\"{enzyme}_{measurement_type}\"])\n",
    "            loss_summary += f\"\\n{label} set loss/$\\sigma$ {measurement_type}:{enzyme}: {mean_sigma}$\\pm${std_dev_of_sigma}\"\n",
    "\n",
    "    ax[1,0].boxplot(all_std_devs[:4])\n",
    "    ax[1,0].set_xticklabels(x_ticklabels[:4])\n",
    "    ax[1,0].set_title(f\"{label} set loss/ ensemble std.dev pIC50\")\n",
    "    \n",
    "    ax[1,1].boxplot(all_std_devs[4:])\n",
    "    ax[1,1].set_xticklabels(x_ticklabels[4:])\n",
    "    ax[1,1].set_title(f\"{label} set loss/ ensemble std.dev pIC50\")\n",
    "    plt.savefig(\"../assets/evaluation_summary.png\")\n",
    "    plt.show()\n",
    "\n",
    "print(loss_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b6b370",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
