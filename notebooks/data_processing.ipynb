{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e83e253",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "\n",
    "![data flow](../assets/data_flow.png)\n",
    "\n",
    "## Establishing a Test Set\n",
    "\n",
    "There is often some overlap in the terms \"test set\" and \"validation set\" in everyday usage by ML practitioners. I am firmly of the opinion that there is a stark and important difference between the two. The test set should not be exposed to models or developers until the model is essentially finished. Any subsequent iterations of the model that are developed after evaluating on the test set necessarily involve some amount of human-mediated data leakage, which can lead to test set overfitting for popular datasets like MNIST. \n",
    "\n",
    "The validation set, on the other hand, can be split out from the training data set and it may be desirable to have several training runs with different validation splits (_i.e._ k-fold cross-validation). The function `split_by_smiles` randomly splits the original DataFrame csv and runs an assertion check to make sure that none of the molecules in the test set are found in the training set. \n",
    "\n",
    "## SMILES sequences to vectors (torch tensors)\n",
    "\n",
    "My approach to the problem involves training an autoregressive transformer on the SMILES sequences directly. The first part of this data parsing process is to convert the string sequences into something we can run through the forward pass of a `torch.nn.Module`. The steps are\n",
    "\n",
    "0. Generate a character vocabulary of all the characters we find in the SMILES sequences, and use a dictionary to assign each character an integer index\n",
    "1. Convert the SMILES sequences to a one hot encoding with dimensions batch size by sequence length by number of token classes. \n",
    "\n",
    "Although I experimenter with incorporating a one hot word2vec vectorizer model based on the Nonlinear Independent Components Estimation (NICE) method, in the end I trained a transformer on reconstructing the one hot vectors from masked versions directly. The NICE method is interesting in that it is reversible, despite being a nonlinear transformation, but it didn't make it into the final training process this time. (See [Faury _et al._ 2019](https://arxiv.org/abs/1901.11271) and [Dinh _et al._ 2014](https://arxiv.org/abs/1410.8516) for more details on NICE). \n",
    "\n",
    "Converting strings to one hot vectors takes some time, so I pre-processed the data and saved the results to disk. This was handled in a dataloader class `SeqDataLoader` that I wrote for a different GNN and transformer project, Sepia. The original version of this process used my low level (`jax.numpy`) transformer implementation in Sepia as well, but I switched to using PyTorch as development in Sepia has been a bit slow.\n",
    "\n",
    "## Encoding features from SMILES sequences with pre-trained transformer\n",
    "\n",
    "After training the transformer, I used it as a feature extracter to provide a dataset for training an ensemble of MLPS to predict pKi and pIC50 values for JAK and TYK kinase enzymes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd410ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_smiles(df, test_percentage = 0.1, my_seed=13):\n",
    "    \"\"\"\n",
    "    splits a pandas dataframe according to the 'SMILES' column\n",
    "    _i.e._ there should be no overlap of molecules in the training and test datasets.     \n",
    "    \"\"\"\n",
    "\n",
    "    npr.seed(my_seed)\n",
    "    \n",
    "    unique_smiles = df[\"SMILES\"].unique()\n",
    "    split_index = npr.rand(*unique_smiles.shape) <= test_percentage\n",
    "    test_smiles = unique_smiles[split_index]\n",
    "    train_val_smiles = unique_smiles[1 - split_index]\n",
    "\n",
    "    test_index = [elem in test_smiles for elem in df[\"SMILES\"]]\n",
    "    train_val_index = [elem not in test_smiles for elem in df[\"SMILES\"]]\n",
    "\n",
    "    shared_indices = np.sum(1.0 * np.array(train_val_index) * 1.0*np.array(test_index))\n",
    "    \n",
    "    assert shared_indices == 0, f\"something went wrong, {shared_indices} shared indices (test data leak)\"\n",
    "    \n",
    "    test_df = df.loc[test_index]\n",
    "    train_df = df.loc[train_val_index]\n",
    "    print(train_df.head())\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f99f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, this code works as part of the library at github.com/Cogibra/Sepia\n",
    "# running this cell alone won't replicate the functionality I used to pre-process the dataset\n",
    "# unless you first install Sepia\n",
    "\n",
    "####\n",
    "# with your virtualenv/conda env/ your favorite env manager env activated\n",
    "#\n",
    "# git clone git@github.com:Cogibra/Sepia.git\n",
    "# cd Sepia\n",
    "# pip install -e .\n",
    "# you'll also need jax, and depending on whether you want to use cuda with jax \n",
    "# installation can be tricky (cuda and jax versions must match) checkout github.com/google/jax for install instructions\n",
    "####\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from sepia.common import query_kwargs\n",
    "import sepia.optimizer as optimizer\n",
    "\n",
    "from sepia.seq.data import \\\n",
    "        aa_keys, \\\n",
    "        make_sequence_dict, \\\n",
    "        make_token_dict, \\\n",
    "        tokens_to_one_hot, \\\n",
    "        compose_batch_tokens_to_one_hot, \\\n",
    "        one_hot_to_sequence, \\\n",
    "        vectors_to_sequence, \\\n",
    "        sequence_to_vectors,\\\n",
    "        batch_sequence_to_vectors\n",
    "\n",
    "# parameters (namedtuples)\n",
    "from sepia.seq.functional import \\\n",
    "        NICEParametersWB, \\\n",
    "        NICEParametersW, \\\n",
    "        SelfAttentionWB, \\\n",
    "        SelfAttentionW, \\\n",
    "        EncodedAttentionW, \\\n",
    "        EncoderParams, \\\n",
    "        DecoderParams, \\\n",
    "        make_layers_tuple, \\\n",
    "        MLPParams \n",
    "\n",
    "# functions\n",
    "from sepia.seq.functional import \\\n",
    "        encoder, \\\n",
    "        decoder, \\\n",
    "        bijective_forward, \\\n",
    "        bijective_reverse\n",
    "\n",
    "from sepia.seq.data import \\\n",
    "        make_sequence_dict, \\\n",
    "        vectors_to_sequence, \\\n",
    "        sequence_to_vectors\n",
    "\n",
    "\n",
    "class SeqDataLoader():\n",
    "\n",
    "    def __init__(self, token_dict: dict, seq_length: int, token_dim: int,\\\n",
    "            **kwargs: dict):\n",
    "\n",
    "        self.token_dict = token_dict\n",
    "        self.seq_length = seq_length\n",
    "        self.token_dim = token_dim\n",
    "\n",
    "        self.shuffle = query_kwargs(\"shuffle\", False, **kwargs)\n",
    "        self.batch_size = query_kwargs(\"batch_size\", 8, **kwargs)\n",
    "        self.my_seed = query_kwargs(\"seed\", 13, **kwargs)\n",
    "\n",
    "        if \"dataset\" in kwargs.keys():\n",
    "            self.setup_dataset(kwargs[\"dataset\"])\n",
    "\n",
    "    def setup_dataset(self, dataset: np.array):\n",
    "        # shape dataset and convert to one hot vectors\n",
    "        # dataset is expected to be a 1D array of string sequences\n",
    "\n",
    "        if type(dataset) == list:\n",
    "            dataset = np.array(dataset)\n",
    "\n",
    "        if self.shuffle:\n",
    "            pass\n",
    "\n",
    "        remainder = dataset.shape[0] % self.batch_size\n",
    "\n",
    "        while remainder:\n",
    "\n",
    "            if remainder:\n",
    "                append_index = self.batch_size - remainder\n",
    "                dataset = np.append(dataset, dataset[0:append_index], axis=0)\n",
    "\n",
    "            remainder = dataset.shape[0] % self.batch_size\n",
    "\n",
    "        dataset = dataset.reshape(-1)\n",
    "\n",
    "        token_dataset = batch_sequence_to_vectors(dataset, self.token_dict,\\\n",
    "                pad_to = self.seq_length)\n",
    "\n",
    "        batch_to_one_hot = compose_batch_tokens_to_one_hot(\\\n",
    "                pad_to = self.seq_length, pad_classes_to = self.token_dim)\n",
    "        one_hot_dataset = batch_to_one_hot(token_dataset)\n",
    "\n",
    "        self.dataset = one_hot_dataset.reshape(-1, self.batch_size, \\\n",
    "                self.seq_length, self.token_dim)\n",
    "\n",
    "    def set_dataset(self, dataset: jnp.array):\n",
    "\n",
    "        remainder = dataset.shape[1] % self.batch_size\n",
    "\n",
    "        while remainder:\n",
    "\n",
    "            if remainder:\n",
    "                append_index = self.batch_size - remainder\n",
    "                dataset = np.append(dataset, dataset[0:append_index], axis=0)\n",
    "\n",
    "            remainder = dataset.shape[0] % self.batch_size\n",
    "\n",
    "\n",
    "        assert self.seq_length == dataset.shape[-2], f\"seq_length {self.seq_length} != {dataset.shape[-2]}\"\n",
    "        assert self.token_dim == dataset.shape[-1], f\"token_dim {self.token_dim} != {dataset.shape[-1]}\"\n",
    "\n",
    "        self.dataset = dataset.reshape(-1, self.batch_size, \\\n",
    "                self.seq_length, self.token_dim)\n",
    "\n",
    "    def save_dataset(self, filepath: str=None):\n",
    "\n",
    "        if filepath is None:\n",
    "            filepath = os.path.join(\"data\", \"temp.npy\")\n",
    "\n",
    "        jnp.save(filepath, self.dataset)\n",
    "\n",
    "    def load_dataset(self, filepath: str=None):\n",
    "\n",
    "        if filepath is None:\n",
    "            filepath = os.path.join(\"data\", \"temp.npy\")\n",
    "\n",
    "        if os.path.exists(filepath):\n",
    "            self.set_dataset(jnp.load(filepath))\n",
    "        else:\n",
    "            print(f\"warning, {filepath} does not exist\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index) -> jnp.array:\n",
    "\n",
    "        return self.dataset[index:index+1]\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        if self.shuffle:\n",
    "            pass\n",
    "\n",
    "        return iter(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879402af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
