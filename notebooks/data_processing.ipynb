{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e83e253",
   "metadata": {},
   "source": [
    "# Converting SMILES sequences to tokens, tokens to one hot encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from jak.transformer import XFormer, xformer_train\n",
    "\n",
    "from jak.data import make_token_dict, \\\n",
    "        sequence_to_vectors, \\\n",
    "        one_hot_to_sequence, \\\n",
    "        tokens_to_one_hot\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd410ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_smiles(df, test_percentage = 0.1, my_seed=13):\n",
    "    \"\"\"\n",
    "    splits a pandas dataframe according to the 'SMILES' column\n",
    "    _i.e._ there should be no overlap of molecules in the training and test datasets.     \n",
    "    \"\"\"\n",
    "\n",
    "    npr.seed(my_seed)\n",
    "    \n",
    "    unique_smiles = df[\"SMILES\"].unique()\n",
    "    split_index = npr.rand(*unique_smiles.shape) <= test_percentage\n",
    "    test_smiles = unique_smiles[split_index]\n",
    "    train_val_smiles = unique_smiles[1 - split_index]\n",
    "\n",
    "    test_index = [elem in test_smiles for elem in df[\"SMILES\"]]\n",
    "    train_val_index = [elem not in test_smiles for elem in df[\"SMILES\"]]\n",
    "\n",
    "    shared_indices = np.sum(1.0 * np.array(train_val_index) * 1.0*np.array(test_index))\n",
    "    \n",
    "    assert shared_indices == 0, f\"something went wrong, {shared_indices} shared indices (test data leak)\"\n",
    "    \n",
    "    test_df = df.loc[test_index]\n",
    "    train_df = df.loc[train_val_index]\n",
    "    print(train_df.head())\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545952ef-327b-41ed-982e-ef3e058cc0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_vocab = \"#()+-1234567=BCFHINOPS[]cilnors\"\n",
    "token_dict = make_token_dict(smiles_vocab)\n",
    "df = pd.read_csv(\"../data/train_JAK.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f99f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, this code works as part of the library at github.com/Cogibra/Sepia\n",
    "# running this cell alone won't replicate the functionality I used to pre-process the dataset\n",
    "# unless you first install Sepia\n",
    "\n",
    "####\n",
    "# with your virtualenv/conda env/ your favorite env manager env activated\n",
    "#\n",
    "# git clone git@github.com:Cogibra/Sepia.git\n",
    "# cd Sepia\n",
    "# pip install -e .\n",
    "# you'll also need jax, and depending on whether you want to use cuda with jax \n",
    "# installation can be tricky (cuda and jax versions must match) checkout github.com/google/jax for install instructions\n",
    "####\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from sepia.common import query_kwargs\n",
    "import sepia.optimizer as optimizer\n",
    "\n",
    "from sepia.seq.data import \\\n",
    "        aa_keys, \\\n",
    "        make_sequence_dict, \\\n",
    "        make_token_dict, \\\n",
    "        tokens_to_one_hot, \\\n",
    "        compose_batch_tokens_to_one_hot, \\\n",
    "        one_hot_to_sequence, \\\n",
    "        vectors_to_sequence, \\\n",
    "        sequence_to_vectors,\\\n",
    "        batch_sequence_to_vectors\n",
    "\n",
    "# parameters (namedtuples)\n",
    "\"\"\"\n",
    "from sepia.seq.functional import \\\n",
    "        NICEParametersWB, \\\n",
    "        NICEParametersW, \\\n",
    "        SelfAttentionWB, \\\n",
    "        SelfAttentionW, \\\n",
    "        EncodedAttentionW, \\\n",
    "        EncoderParams, \\\n",
    "        DecoderParams, \\\n",
    "        make_layers_tuple, \\\n",
    "        MLPParams \n",
    "\n",
    "# functions\n",
    "from sepia.seq.functional import \\\n",
    "        encoder, \\\n",
    "        decoder, \\\n",
    "        bijective_forward, \\\n",
    "        bijective_reverse\n",
    "\"\"\"\n",
    "from sepia.seq.data import \\\n",
    "        make_sequence_dict, \\\n",
    "        vectors_to_sequence, \\\n",
    "        sequence_to_vectors\n",
    "\n",
    "\n",
    "class SeqDataLoader():\n",
    "\n",
    "    def __init__(self, token_dict: dict, seq_length: int, token_dim: int,\\\n",
    "            **kwargs: dict):\n",
    "\n",
    "        self.token_dict = token_dict\n",
    "        self.seq_length = seq_length\n",
    "        self.token_dim = token_dim\n",
    "\n",
    "        self.shuffle = query_kwargs(\"shuffle\", False, **kwargs)\n",
    "        self.batch_size = query_kwargs(\"batch_size\", 8, **kwargs)\n",
    "        self.my_seed = query_kwargs(\"seed\", 13, **kwargs)\n",
    "\n",
    "        if \"dataset\" in kwargs.keys():\n",
    "            self.setup_dataset(kwargs[\"dataset\"])\n",
    "\n",
    "    def setup_dataset(self, dataset: np.array):\n",
    "        # shape dataset and convert to one hot vectors\n",
    "        # dataset is expected to be 1D np.array of string sequences\n",
    "        # (can also convert from list)\n",
    "\n",
    "        if type(dataset) == list:\n",
    "            dataset = np.array(dataset)\n",
    "\n",
    "        if self.shuffle:\n",
    "            pass\n",
    "\n",
    "        remainder = dataset.shape[0] % self.batch_size\n",
    "\n",
    "        while remainder:\n",
    "\n",
    "            if remainder:\n",
    "                append_index = self.batch_size - remainder\n",
    "                dataset = np.append(dataset, dataset[0:append_index], axis=0)\n",
    "\n",
    "            remainder = dataset.shape[0] % self.batch_size\n",
    "\n",
    "        dataset = dataset.reshape(-1)\n",
    "\n",
    "        token_dataset = batch_sequence_to_vectors(dataset, self.token_dict,\\\n",
    "                pad_to = self.seq_length)\n",
    "\n",
    "        batch_to_one_hot = compose_batch_tokens_to_one_hot(\\\n",
    "                pad_to = self.seq_length, pad_classes_to = self.token_dim)\n",
    "        one_hot_dataset = batch_to_one_hot(token_dataset)\n",
    "\n",
    "        self.dataset = one_hot_dataset.reshape(-1, self.batch_size, \\\n",
    "                self.seq_length, self.token_dim)\n",
    "\n",
    "    def set_dataset(self, dataset: jnp.array):\n",
    "\n",
    "        remainder = dataset.shape[1] % self.batch_size\n",
    "\n",
    "        while remainder:\n",
    "\n",
    "            if remainder:\n",
    "                append_index = self.batch_size - remainder\n",
    "                dataset = np.append(dataset, dataset[0:append_index], axis=0)\n",
    "\n",
    "            remainder = dataset.shape[0] % self.batch_size\n",
    "\n",
    "\n",
    "        assert self.seq_length == dataset.shape[-2], f\"seq_length {self.seq_length} != {dataset.shape[-2]}\"\n",
    "        assert self.token_dim == dataset.shape[-1], f\"token_dim {self.token_dim} != {dataset.shape[-1]}\"\n",
    "\n",
    "        self.dataset = dataset.reshape(-1, self.batch_size, \\\n",
    "                self.seq_length, self.token_dim)\n",
    "\n",
    "    def save_dataset(self, filepath: str=None, reshape: bool=True):\n",
    "\n",
    "        if filepath is None:\n",
    "            filepath = os.path.join(\"data\", \"temp.npy\")\n",
    "        if reshape:\n",
    "            save_dataset = self.dataset.reshape(-1, *self.dataset.shape[-2:])\n",
    "        else:\n",
    "            save_dataset = self.dataset\n",
    "\n",
    "        jnp.save(filepath, save_dataset)\n",
    "\n",
    "    def load_dataset(self, filepath: str=None):\n",
    "\n",
    "        if filepath is None:\n",
    "            filepath = os.path.join(\"data\", \"temp.npy\")\n",
    "\n",
    "        if os.path.exists(filepath):\n",
    "            self.set_dataset(jnp.load(filepath))\n",
    "        else:\n",
    "            print(f\"warning, {filepath} does not exist\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index) -> jnp.array:\n",
    "\n",
    "        return self.dataset[index:index+1]\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        if self.shuffle:\n",
    "            pass\n",
    "\n",
    "        return iter(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc05e890-49b6-49cf-a137-469122af41d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(df[\"SMILES\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b633feb1-8138-467e-a173-e3f5fb7e17aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_vocab = \"#()+-1234567=BCFHINOPS[]cilnors\"\n",
    "token_dict = make_token_dict(smiles_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879402af",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_vocab = \"#()+-1234567=BCFHINOPS[]cilnors\"\n",
    "token_dict = make_token_dict(smiles_vocab)\n",
    "df = pd.read_csv(\"../data/train_JAK.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148b76ea-29b9-4b1f-b587-6a4192b91d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(df[\"SMILES\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cdf468-0235-4650-a6a2-34e4f23d9ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdl = SeqDataLoader(token_dict=token_dict, seq_length=128, token_dim=36, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3026e786-5f7f-4e1b-8e38-569a06600822",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdl.setup_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f732604-220a-46f5-bed9-7b5e250e0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filename = \"jak_one_hot_smiles.npy\"\n",
    "sdl.save_dataset(os.path.join(\"..\", \"data\", dataset_filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
