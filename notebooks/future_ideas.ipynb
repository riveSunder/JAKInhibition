{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89cf3f69",
   "metadata": {},
   "source": [
    "# Future Ideas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd770fe",
   "metadata": {},
   "source": [
    "### Baselines\n",
    "\n",
    "It's tempting to approach every problem with the latest or at least a recent deep learning technique, and in keeping the theme for this project that is the approach I took here. Often these work well, but it's important to validate these techniques against more mature and mundane baselines. Recently, I've spent a lot of time using evolutionary methods for reinforcement learning-type control problems, and for those I typically use a random search method as a baseline. This type of baseline provides a useful sanity check that can tell you if the problem is too easy or your method is more complicated than it needs to be. \n",
    "\n",
    "For this project I would next work on establishing some baselines using statistical learning techniques (I typically use `sklearn` for reliable implementations). \n",
    "\n",
    "### Hyperparameter search in the current process\n",
    "\n",
    "Given that the process I used has two main parts (unsupervised training of a transformer, regression training for inhibition metrics), I would perform hyperparameter search separately. I often use the [covariance matrix adaptation evolution strategy](https://www.semanticscholar.org/paper/The-CMA-Evolution-Strategy%3A-A-Tutorial-Hansen/7c6409ec154ba64f5eb63d8c6e9f419ce1472289) (CMA-ES) for experiments in reinforcement learning and complex systems, and CMA-ES is well suited for hyperparameter search as well. CMA-ES can be slow for evolving models with more than a few thousand parameters (the multivariate sampling step is the bottleneck) but it is a good choice for optimizing the ~10s parameters in a typical hyperparameter tuning context. \n",
    "\n",
    "### Autoencoder loss as another metric for estimating uncertainty\n",
    "\n",
    "During regression training, train an autoencoder to replicate the features (encoded by the transformer). Low autoencoder loss corresponds to more confident predictions, and autoencoder loss more than ~ 2 to 5 standard deviations higher than the mean could be taken as an indicator not to trust a given prediction. \n",
    "\n",
    "### A multiheaded graph neural network for extracting features and pKi/pIC50 regression.\n",
    "\n",
    "For this project, I decided to try unsupervised training with a transformer as a feature extractor, and to use the features encoded by the transformer to train MLPs. While training on SMILES sequence doesn't through away the structural connection data of the molecules represented, it does rely on the transformer learning something about the molecular graph structure on its own. \n",
    "\n",
    "Unsupervised learning can work pretty well for learning structural information from sequence data, particularly in a domain such as protein or nucleotide sequences, where there is vastly more sequence data than structure data and plenty of samples to learn from (_e.g._ [Alley _et al._](https://www.biorxiv.org/content/10.1101/589333v1) or [Brandes _et al._](https://academic.oup.com/bioinformatics/article/38/8/2102/6502274?login=false)). In this project the dataset has only a few thousand molecules represented in SMILES format, and it makes sense to impart our models with every advantage we can in the form of inductive biases, namely the graph-structure information that can be parsed from the SMILES sequences. \n",
    "\n",
    "In retrospect a GNN approach, outlined below, may \n",
    "\n",
    "* parse SMILES to graph tuples \n",
    "* Using a semi-supervised approach, train GNNS to infer missing nodes that have been masked in the input data.\n",
    "* Adding a regression head to the GNN, train on predicting kinase inhibition measurements, starting with a GNN pre-trained for inferring missing node features.  \n",
    "\n",
    "Alternatively, we could enlarge the pre-training dataset...\n",
    "\n",
    "### Pre-train on larger SMILES dataset\n",
    "\n",
    "The JAK kinase dataset contains ~4000 different molecules for which pIC50 or pKi has been measured on one or more kinase. For more effective pre-training, a larger dataset gives a large language model a better chance of learning the patterns associated with structural relationships encoded in SMILES. \n",
    "\n",
    "Pre-training data also does not need to be labeled to be useful, so we could use a large dataset, such as [GDB-17](https://www.gdb.unibe.ch/downloads/) or one of its smaller siblings, to pre-train on millions (to billions) of SMILES sequences instead of the ~4000 present in the kinase dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d405c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
